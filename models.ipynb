{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.88\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Bream       1.00      1.00      1.00        10\n",
      "      Parkki       1.00      1.00      1.00         1\n",
      "       Perch       0.69      1.00      0.82         9\n",
      "        Pike       1.00      1.00      1.00         3\n",
      "       Roach       0.00      0.00      0.00         1\n",
      "       Smelt       1.00      1.00      1.00         5\n",
      "   Whitefish       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.88        32\n",
      "   macro avg       0.67      0.71      0.69        32\n",
      "weighted avg       0.79      0.88      0.82        32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sehre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\sehre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\sehre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('fish.csv')\n",
    "\n",
    "X = data.drop(columns=['Species'])\n",
    "\n",
    "# Target variable ('Species')\n",
    "y = data['Species']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report_str = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Accuracy**: The overall accuracy of the classification model is **0.88** (or 88%). This means that approximately 88% of the predictions made by the model are correct.\n",
    "\n",
    "2. **Precision**:\n",
    "   - Precision measures the proportion of true positive predictions (correctly predicted instances) out of all positive predictions (both true positives and false positives).\n",
    "   - For each class (species):\n",
    "       - Bream: 100% precision (all predicted Bream instances are correct).\n",
    "       - Parkki: 100% precision.\n",
    "       - Perch: 69% precision (some false positives).\n",
    "       - Pike: 100% precision.\n",
    "       - Roach: 0% precision (all predicted Roach instances are incorrect).\n",
    "       - Smelt: 100% precision.\n",
    "       - Whitefish: 0% precision.\n",
    "   - Weighted average precision: 79%\n",
    "\n",
    "3. **Recall (Sensitivity)**:\n",
    "   - Recall measures the proportion of true positive predictions out of all actual positive instances.\n",
    "   - For each class:\n",
    "       - Bream: 100% recall (all actual Bream instances are correctly predicted).\n",
    "       - Parkki: 100% recall.\n",
    "       - Perch: 100% recall (no false negatives).\n",
    "       - Pike: 100% recall.\n",
    "       - Roach: 0% recall (all actual Roach instances are missed).\n",
    "       - Smelt: 100% recall.\n",
    "       - Whitefish: 0% recall (all actual Whitefish instances are missed).\n",
    "   - Weighted average recall: 88%\n",
    "\n",
    "4. **F1-Score**:\n",
    "   - The F1-score balances precision and recall, providing a single metric.\n",
    "   - It considers both false positives and false negatives.\n",
    "   - Weighted average F1-score: 82%\n",
    "\n",
    "5. **Support**:\n",
    "   - The number of instances (samples) for each class.\n",
    "   - For example, there are 10 instances of Bream, 1 instance of Parkki, and so on.\n",
    "\n",
    "6. **Macro Average**:\n",
    "   - The average precision, recall, and F1-score across all classes (unweighted).\n",
    "   - Macro average precision: 67%\n",
    "   - Macro average recall: 71%\n",
    "   - Macro average F1-score: 69%\n",
    "\n",
    "7. **Weighted Average**:\n",
    "   - The average precision, recall, and F1-score, weighted by the number of instances in each class.\n",
    "   - Weighted average precision: 79%\n",
    "   - Weighted average recall: 88%\n",
    "   - Weighted average F1-score: 82%\n",
    "\n",
    "In summary, the model performs well in some classes (e.g., Bream, Parkki, Pike, Smelt) but struggles with others (e.g., Roach, Whitefish). Improving precision and recall for the challenging classes would enhance overall model performance. üìäüêü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Bream       1.00      0.88      0.93        16\n",
      "      Parkki       0.71      1.00      0.83         5\n",
      "       Perch       1.00      0.29      0.45        17\n",
      "        Pike       1.00      1.00      1.00        10\n",
      "       Roach       0.44      0.80      0.57        10\n",
      "       Smelt       0.78      1.00      0.88         7\n",
      "   Whitefish       0.75      0.86      0.80        14\n",
      "\n",
      "    accuracy                           0.77        79\n",
      "   macro avg       0.81      0.83      0.78        79\n",
      "weighted avg       0.85      0.77      0.76        79\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE  # For addressing class imbalance\n",
    "\n",
    "# Features (excluding 'Species')\n",
    "X = data.drop(columns=['Species'])\n",
    "\n",
    "# Target variable ('Species')\n",
    "y = data['Species']\n",
    "\n",
    "# Address class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report_str = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Accuracy (Overall Performance)**:\n",
    "   - The overall accuracy of the model is **0.77** (or 77%).\n",
    "   - This means that approximately 77% of the predictions made by the model are correct.\n",
    "\n",
    "2. **Precision**:\n",
    "   - Precision measures the proportion of true positive predictions (correctly predicted instances) out of all positive predictions (both true positives and false positives).\n",
    "   - For each class (species):\n",
    "       - Bream: 100% precision (all predicted Bream instances are correct).\n",
    "       - Parkki: 71% precision.\n",
    "       - Perch: 100% precision (some false positives).\n",
    "       - Pike: 100% precision.\n",
    "       - Roach: 44% precision (some false positives).\n",
    "       - Smelt: 78% precision.\n",
    "       - Whitefish: 75% precision.\n",
    "\n",
    "3. **Recall (Sensitivity)**:\n",
    "   - Recall measures the proportion of true positive predictions out of all actual positive instances.\n",
    "   - For each class:\n",
    "       - Bream: 88% recall (some actual Bream instances are missed).\n",
    "       - Parkki: 100% recall.\n",
    "       - Perch: 29% recall (many false negatives).\n",
    "       - Pike: 100% recall.\n",
    "       - Roach: 80% recall.\n",
    "       - Smelt: 100% recall.\n",
    "       - Whitefish: 86% recall.\n",
    "\n",
    "4. **F1-Score**:\n",
    "   - The F1-score balances precision and recall, providing a single metric.\n",
    "   - It considers both false positives and false negatives.\n",
    "   - Weighted average F1-score: 76%\n",
    "\n",
    "5. **Macro Average**:\n",
    "   - The average precision, recall, and F1-score across all classes (unweighted).\n",
    "   - Macro average precision: 81%\n",
    "   - Macro average recall: 83%\n",
    "   - Macro average F1-score: 78%\n",
    "\n",
    "6. **Weighted Average**:\n",
    "   - The average precision, recall, and F1-score, weighted by the number of instances in each class.\n",
    "   - Weighted average precision: 85%\n",
    "   - Weighted average recall: 77%\n",
    "   - Weighted average F1-score: 76%\n",
    "\n",
    "In summary, the model's performance has slightly decreased compared to the previous accuracy of 0.88. However, this makes the model more generalized. It still performs well for some classes but struggles with others. Improving recall for classes like Perch and Roach could enhance overall model effectiveness. üìäüêü"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 83.71 grams\n"
     ]
    }
   ],
   "source": [
    "# Features (excluding 'Weight')\n",
    "X_reg = data.drop(columns=['Weight'])\n",
    "\n",
    "# Convert categorical variables into numerical form\n",
    "X_reg = pd.get_dummies(X_reg, columns=['Species'])\n",
    "\n",
    "# Target variable ('Weight')\n",
    "y_reg = data['Weight']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear regression model\n",
    "reg_model = LinearRegression()\n",
    "reg_model.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Make weight predictions\n",
    "y_pred_reg = reg_model.predict(X_test_reg)\n",
    "\n",
    "# Evaluate model performance (Root Mean Squared Error)\n",
    "rmse = mean_squared_error(y_test_reg, y_pred_reg, squared=False)\n",
    "\n",
    "print(f\"Root Mean Squared Error: {rmse:.2f} grams\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "def save_model(model, model_filename):\n",
    "    joblib.dump(model, model_filename)\n",
    "    print(f\"Model saved as {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as logistic_regression.pkl\n",
      "Model saved as linear_regression.pkl\n"
     ]
    }
   ],
   "source": [
    "save_model(model, 'logistic_regression.pkl')\n",
    "save_model(reg_model, 'linear_regression.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
